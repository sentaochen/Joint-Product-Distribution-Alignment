{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b73b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy.linalg as la\n",
    "from sklearn.preprocessing import scale,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af789eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size=1000,hidden_size=100,output_size=10):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=hidden_size, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=hidden_size, out_features=output_size, bias=True)\n",
    "    def forward(self,X):\n",
    "        FX = F.relu(self.fc1(X)) # hidden layer activation features\n",
    "        prob = F.softmax(self.fc2(FX),dim=1) # probability output\n",
    "        return FX,prob\n",
    "    \n",
    "def RCS(FX,y,l,alpha=0.5,sigma=100,lamda=1e-2):        \n",
    "    m = FX.shape[0]    \n",
    "    Deltay = torch.as_tensor(y[:,None]==y,dtype=torch.float64,device=FX.device)\n",
    "    Deltal = torch.as_tensor(l[:,None]==l,dtype=torch.float64,device=FX.device)\n",
    "    FX_norm = torch.sum(FX ** 2, axis = -1)\n",
    "    K = torch.exp(-(FX_norm[:,None] + FX_norm[None,:] - 2 * torch.matmul(FX, FX.t())) / sigma) * Deltay\n",
    "                    \n",
    "    P = K * Deltal\n",
    "    H = ((1.0 - alpha) / m**2) * torch.matmul(K,K) * torch.matmul(Deltal,Deltal) + (1.0 * alpha / m) * torch.matmul(P,P)\n",
    "    h = torch.mean(P,axis=0)\n",
    "    theta = torch.matmul(torch.inverse(H + lamda * torch.eye(m,device=FX.device)),h)\n",
    "    D = 2 * torch.matmul(h,theta) - torch.matmul(theta,torch.matmul(H,theta)) - 1\n",
    "    return D\n",
    "\n",
    "\n",
    "class JPDA:\n",
    "    \"\"\"\n",
    "    The true training batch size per iteration is batch_size * num_class * num_domain\n",
    "    \"\"\"\n",
    "    def __init__(self,input_size=1024,hidden_size=512,output_size=68,seed=1000,device=torch.device('cpu'),\n",
    "                 epoch=200,alpha=0.5,sigma=None,lamda=1e-2,gamma=1,batch_size=4,lr=1e-3,log=False):\n",
    "        args_values = locals()\n",
    "        args_values.pop(\"self\")\n",
    "        for arg,value in args_values.items():\n",
    "            setattr(self,arg,value)\n",
    "            \n",
    "    def fit(self,X_list,y_list,Xt,yt):\n",
    "        torch.manual_seed(self.seed)\n",
    "        net = NeuralNet(input_size=self.input_size,hidden_size=self.hidden_size,output_size=self.output_size).to(self.device)\n",
    "        optimizer = optim.SGD(params=net.parameters(),lr=self.lr,momentum=0.9)\n",
    "        \n",
    "        for epoch in range(self.epoch):\n",
    "            dataset_loaders,l = [],0\n",
    "            for Xs,ys in zip(X_list,y_list):\n",
    "                for i,counts in zip(*np.unique(ys,return_counts=True)):\n",
    "                    dataset = np.hstack((Xs[ys==i],ys[ys==i][:,None],l * np.ones((counts,1))))\n",
    "                    dataset_loaders.append(torch.utils.data.DataLoader(dataset=torch.tensor(dataset),batch_size=self.batch_size,shuffle=True,drop_last=True))\n",
    "                l = l + 1\n",
    "\n",
    "            train_err,m_train = 0.0, 0.0\n",
    "\n",
    "            for batches in zip(*dataset_loaders):\n",
    "                Xyl = torch.cat(batches,dim=0)\n",
    "                X,y,l = Xyl[:,:-2].to(self.device,torch.float32),Xyl[:,-2].to(self.device,torch.int64),Xyl[:,-1].to(self.device,torch.int64)\n",
    "\n",
    "                m = X.shape[0]\n",
    "                FX,prob = net(X)\n",
    "    \n",
    "                num_class = prob.shape[1]\n",
    "                negative_log_loss = -torch.mean(torch.sum(torch.log(prob) * F.one_hot(y,num_class),dim=1))\n",
    "            \n",
    "                if self.sigma is None:\n",
    "                    pairwise_dist = torch.cdist(X,X,p=2)**2 \n",
    "                    self.sigma = torch.median(pairwise_dist[pairwise_dist!=0]) \n",
    "                    \n",
    "                rcs_loss = RCS(FX,y,l,alpha=self.alpha,sigma=self.sigma,lamda=self.lamda)\n",
    "                loss = negative_log_loss + self.gamma * rcs_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_err += loss.item() * m\n",
    "                m_train += m\n",
    "\n",
    "            with torch.no_grad():\n",
    "                Xt,yt = torch.as_tensor(Xt,dtype=torch.float32,device=self.device),torch.as_tensor(yt,dtype=torch.int64,device=self.device)    \n",
    "                pred = torch.argmax(net(Xt)[1],dim=1)\n",
    "                correct = torch.sum((pred == yt)).item()\n",
    "                m_test = len(yt)\n",
    "            \n",
    "            if True == self.log:\n",
    "                print('epoch ',epoch,', training error ',train_err / m_train,', test acc. ',(correct / m_test) * 100) \n",
    "        self.net = net\n",
    "        \n",
    "    def score(self,Xt,yt):\n",
    "        with torch.no_grad():\n",
    "            Xt,yt = torch.as_tensor(Xt,dtype=torch.float32,device=self.device),torch.as_tensor(yt,dtype=torch.int64,device=self.device)    \n",
    "            pred = torch.argmax(self.net(Xt)[1],dim=1)\n",
    "            correct = torch.sum((pred == yt)).item()\n",
    "            m_test = len(yt)\n",
    "        return (correct / m_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ea2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(tg,domains):\n",
    "    data = sio.loadmat('datasets/PIE/' + tg + '.mat')\n",
    "    Xt,yt = data['fea'].astype(np.float64),data['gnd'].ravel()\n",
    "    yt = LabelEncoder().fit(yt).transform(yt).astype(np.float64)\n",
    "    Xt = scale(Xt / Xt.sum(axis=1,keepdims=True))\n",
    "    \n",
    "    Xs_list,ys_list = [],[]\n",
    "    for sc in domains:\n",
    "        if sc != tg:\n",
    "            data = sio.loadmat('datasets/PIE/' + sc + '.mat')\n",
    "            Xs,ys = data['fea'].astype(np.float64),data['gnd'].ravel()\n",
    "            ys = LabelEncoder().fit(ys).transform(ys).astype(np.float64)\n",
    "            Xs = scale(Xs / Xs.sum(axis=1,keepdims=True))\n",
    "            Xs_list.append(Xs),ys_list.append(ys)        \n",
    "    \n",
    "    return Xs_list,ys_list,Xt,yt\n",
    "\n",
    "domains = ['PIE05','PIE07','PIE09','PIE27','PIE29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439e1211",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.46458583433373"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "X_list,y_list,Xt,yt = readData('PIE05',domains)\n",
    "    \n",
    "instance = JPDA(input_size=1024,hidden_size=512,output_size=68,seed=0,device=DEVICE,\n",
    "                         epoch=300,alpha=0.5,lamda=1e-3,gamma=100,batch_size=5,lr=1e-2,log=False)\n",
    "instance.fit(X_list,y_list,Xt,yt)\n",
    "instance.score(Xt,yt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
